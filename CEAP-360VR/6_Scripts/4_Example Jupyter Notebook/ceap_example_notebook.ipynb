{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Dataset CEAP-360VR with pandas and scikit-learn\n",
    "\n",
    "This notebook loads and preprocesses the dataset `CEAP-360VR` [GitHub repo](https://github.com/luiseduve/CEAP-360VR-Dataset) described in the paper *CEAP-360VR: A Continuous Physiological and\n",
    "Behavioral Emotion Annotation Dataset for 360 VR Videos* [(DOI)](10.1109/TMM.2021.3124080)\n",
    "\n",
    "*Description:*\n",
    "\n",
    "1. A class was created to load the individual Json files in a structured way through the index file `data_tree_index.json`. Similarly, demographics and video stimuli information are stored in two .csv files. The `Frame` data was used as main data source.\n",
    "2. The sampling frequency is different among data modalities, they were normalized to 30Hz for all videos (Video1 was at 25Hz). Moreover, we loaded `Raw` IBI to generate new signals `IBI_R_Peaks` indicating with a 1 when a heart-rate beat was detected. This information is useful for HRV analysis.\n",
    "3. Finally, the dataset is combined to avoid missing values and append the class label according to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceap_loader\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizatio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the files generated from this notebook are in a subfolder with this name\n",
    "STR_DATASET = \"ceap_example_notebookCEAP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_path_temp(filename, subfolders=\"\", extension=\".csv\"):\n",
    "    # Function to generate temporary files easier\n",
    "    TEMP_FOLDER_NAME = \"./temp/\"\n",
    "    return ceap_loader.generate_complete_path(filename, \\\n",
    "                                        main_folder=TEMP_FOLDER_NAME, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load full dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate index\n",
    "\n",
    "The class `DatasetCEAP` generates an index file `data_tree_index.json` that searches the folder path containing a specific type of data for a specific participant. The constructor class also loads the description of the stimuli and demographics data (*not the questionnaires*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if experiment_config.DATASET_MAIN == Datasets.CEAP:\n",
    "dataset_root_folder = \"../../CEAP-360VR/\"\n",
    "print(dataset_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_ceap = ceap_loader.DatasetCEAP(dataset_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_ceap.stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_ceap.demographics.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a single loaded file\n",
    "\n",
    "Test loading a single file and plotting all the data contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CEAP_df(df, time_colname=\"TimeStamp\", y_colname=\"VideoID\"):\n",
    "    \"\"\"\n",
    "    This function plots a single dataframe from the CEAP dataset.\n",
    "    `df` is the result of applying the function `load_data_from_participant()`.\n",
    "\n",
    "    A single file from the dataset contains many features, which may be sampled\n",
    "    at different frequencies. This function takes a large dataset and creates a\n",
    "    subplot (timeStamp, videoId) to generate a plot with the loaded data\n",
    "    \"\"\"\n",
    "    # Get the numeric columns and delete the `index_colname` to make it index later\n",
    "    cols = list(df.select_dtypes([np.number]).columns)\n",
    "    if time_colname not in cols: raise ValueError(f\"The dataframe does not contain numeric columns for the index x_colname={time_colname}\")\n",
    "    if y_colname not in df.columns: raise ValueError(f\"The dataframe does not contain column with y_colname={y_colname}\")\n",
    "    cols.remove(time_colname)\n",
    "    if y_colname in cols: cols.remove(y_colname)\n",
    "\n",
    "    NUM_ROWS = len(cols)\n",
    "    colnames_labels = df[y_colname].unique()\n",
    "    NUM_COLS = len(colnames_labels)\n",
    "\n",
    "    cmap = matplotlib.cm.get_cmap(\"tab20\")\n",
    "    fig,axes = plt.subplots(NUM_ROWS, NUM_COLS, sharex=True, figsize=(6*NUM_COLS, 2*NUM_ROWS))\n",
    "    for i in range(NUM_ROWS):\n",
    "        for j in range(NUM_COLS):\n",
    "            ax = axes[i,j]\n",
    "\n",
    "            # Filter the data that has to do with this column label\n",
    "            df_ax = df[ df[y_colname] == colnames_labels[j] ]\n",
    "            df_ax = df_ax[[time_colname, cols[i]]].set_index(time_colname).dropna(axis=0)\n",
    "            timestamps = df_ax.index.values\n",
    "            data = df_ax[cols[i]].values\n",
    "            ax.plot(timestamps, data, label=cols[i], color=cmap.colors[i] )\n",
    "            if(i==0): ax.set_title(f\"{y_colname}: {colnames_labels[j]}\") # Suptitles for first row\n",
    "            if(j==0): ax.set_ylabel(cols[i])    # Xlabel for first column\n",
    "            if(i==NUM_ROWS-1): ax.set_xlabel(time_colname)\n",
    "    plt.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of data to load\n",
    "pid = 1         # 1-32\n",
    "typ = \"Physio\"  # [\"Annotations\", \"Behavior\", \"Physio\"]\n",
    "prep = \"Frame\"    # [\"Raw\", \"Transformed\", \"Frame\"]\n",
    "\n",
    "# Load data \n",
    "data_loaded = data_loader_ceap.load_data_from_participant(pid,typ,prep)\n",
    "data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Was data missed when applying `merge` of all features from single dataframe?\n",
    "## To know how many samples were loaded per feature on a specific feature\n",
    "ts_colname = \"TimeStamp\"    #data_loader_ceap.K_TIMESTAMP\n",
    "video_colname = \"VideoID\"   #data_loader_ceap.K_VIDEO\n",
    "participant_colname = \"ParticipantID\"\n",
    "\n",
    "cols = list(data_loaded.select_dtypes([np.number]).columns)\n",
    "cols.remove(ts_colname)\n",
    "CHECK_VIDEO_ID = 2\n",
    "for c in cols:\n",
    "    df_ax = data_loaded[ data_loaded[video_colname] == CHECK_VIDEO_ID ]\n",
    "    df_ax = df_ax[[ts_colname, c]].set_index(ts_colname).dropna(axis=0)\n",
    "    print(f\"C:{c} \\t{df_ax.shape}\")\n",
    "\n",
    "#### Desired output from this cell should show:\n",
    "# - The same number of samples for all features (except IBI) if preprocessing type = \"Frame\"\n",
    "# - Different number of samples per feature if preprocessing type = \"Raw\" because every sensor has different sampling freq\n",
    "\n",
    "## CONCLUSION\n",
    "# Merging is working fine!! \n",
    "# - IBI has fewer samples and need to be resampled.\n",
    "# - VideoID 1 was resampled at 25, and the rest at 30Hz. Because video1 had lower FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CEAP_df(data_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data from all file in the dataset\n",
    "\n",
    "**Uncomment if needed** The cell below takes around **130mins** plotting the whole dataset, per loaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate plots per data type and to visualize all the data per participant\n",
    "# for typ in data_loader_ceap.LIST_DATA_TYPES:\n",
    "#     for prep in data_loader_ceap.LIST_PROCESSING_LEVELS:\n",
    "#         for pid in range(1,33):\n",
    "#             data_loaded = data_loader_ceap.load_data_from_participant(pid,typ,prep)\n",
    "#             plot_CEAP_df(data_loaded)\n",
    "#             save_path_plot = gen_path_plot(f\"{prep}/Participant{pid}_{typ}\")\n",
    "#             plt.savefig(save_path_plot)\n",
    "#             plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a CSV with data of interest\n",
    "\n",
    "Creating a CSV for the whole dataset produces a file `~800MB`.\n",
    "\n",
    "Thus, we decided to choose only the data with preprocessing level `Frame`. These files proved to be accurate as accurate as the `Raw` data, but they already normalized and resampled at `30Hz`.\n",
    "\n",
    "The `Raw` data is used to extract the `IBI` and calculate new HRV from them.\n",
    "\n",
    "Finally, the dataset of interest is comprised by:\n",
    "- `Annotations`, `Behavior`, and `Physio` as found in the folder: `Frame`\n",
    "- `IBI` is excluded from the dataset because presents irregular samples.\n",
    "- New `IBI` and `HRV` is calculated from the folder: `Raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants IDS\n",
    "PARTICIPANTS_IDS = np.arange(1,33)\n",
    "# Load data Annotations, Behavior, and Physio\n",
    "DATA_GROUPS = data_loader_ceap.LIST_DATA_TYPES\n",
    "# Load the Raw, Transformed, or Frame (resampled) data processing\n",
    "DATA_PROCESSING_LEVELS = [\"Frame\"]#data_loader_ceap.LIST_PROCESSING_LEVELS\n",
    "print(f\"DATA_GROUPS={DATA_GROUPS}, DATA_PROCESSING_LEVELS={DATA_PROCESSING_LEVELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_FILENAME = gen_path_temp(\"Dataset_CEAP_resampled_by_Frame\", extension=\".csv\")\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "data_postprocessed = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [DATASET_POSTPROCESSED_FILENAME]\n",
    "\n",
    "# Try to load or create files\n",
    "for tries in range(3): # One for loading, and max 2 for creating+loading\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Try # {tries+1} to load files: {input_files}\")\n",
    "        \n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to READ from files\n",
    "        data_postprocessed = pd.read_csv(input_files[0])\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "        #############################################################\n",
    "        break\n",
    "    except Exception as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to WRITE to disk\n",
    "\n",
    "        # Load all data resampled by frame\n",
    "        for pid in PARTICIPANTS_IDS:\n",
    "            for dttype in DATA_GROUPS:\n",
    "                for prep in DATA_PROCESSING_LEVELS:\n",
    "                    df_single_file = data_loader_ceap.load_data_from_participant(pid,dttype,prep)\n",
    "                    data_postprocessed = df_single_file if (data_postprocessed is None) else pd.concat([data_postprocessed, df_single_file], axis=0)\n",
    "            \n",
    "        # Saving .csv\n",
    "        data_postprocessed.to_csv( input_files[0], index=False)\n",
    "\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        print(f\"\\n\\tFinished creating files {input_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_postprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Preprocessing stages\n",
    "---\n",
    "\n",
    "1. Delete the data corresponding to `IBI` because it has different sampling frequency than the rest of the dataset. The `IBI` will be replaced later with a signal indicating the peaks.\n",
    "2. Supersample the `Frame` data from `VideoID=1` from 25 to 30Hz. *Reason:* All data in `Frame` is resampled to the FPS (Frames per second) of the video where the data was collected. However, `VideoID=1` is the only one with @25Hz, whereas all others `VideoID=2..8` are resampled @30Hz. Supersample `V1` to make them consistent across videos.\n",
    "3. Extract heart-rate variability data from the `IBI` column in the type `Raw`. *Reason:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant sampling frequency to be applied to data from Video1 and to transform IBI to peaks.\n",
    "RESAMPLING_FREQUENCY = 30       # What is the sampling frequency of the peaks array?\n",
    "\n",
    "# Name of the column containing IBI data (this column will be removed and replaced by R-peaks)\n",
    "ibi_colname = \"IBI_IBI\"\n",
    "r_peaks_colname = \"IBI_R_Peaks\" # Name that will be used after transforming IBI into R-peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove IBI data\n",
    "\n",
    "Remove all the dataset with IBI, and drop the rows with NA. This cleaning should lead to dataframes that have the same number of samples per participant, video, and data group. (i.e., 1800 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove IBI colnames. \n",
    "# However, we also need to delete the rows full of NaN, that only had values on IBI.\n",
    "data_postprocessed = data_postprocessed.drop([ibi_colname], axis=1)\n",
    "data_postprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the non-numeric `basic` colnames from the `data` colnames containing the relevant time-series.\n",
    "# The rows whose `data_colnames` are all NaN will be removed. Because it was a row created by IBI.\n",
    "basic_cols = [\"data_type\",\"processing_level\", participant_colname, video_colname, ts_colname]\n",
    "\n",
    "# Difference between sets of colnames\n",
    "data_colnames = set(data_postprocessed.columns).difference(set(basic_cols))\n",
    "# Remove all the rows that are empty, after removing IBI data\n",
    "remaining_data_index = data_postprocessed[data_colnames].dropna(axis=0, how=\"all\").index\n",
    "# New post_processed data should have `1800` samples per time-series (except Video1)\n",
    "data_postprocessed = data_postprocessed.loc[remaining_data_index]\n",
    "\n",
    "# After removing IBI, all arrays have 1800 samples\n",
    "df_filtered_single_ts = data_postprocessed[ (data_postprocessed.VideoID == 5) # Do not use VideoID=1 here!\n",
    "                    & (data_postprocessed.ParticipantID == 1)\n",
    "                    & (data_postprocessed.data_type == \"Physio\") ]\n",
    "df_filtered_single_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supersample `Frame` data from VideoID=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_df_with_interpolation(df, new_timestamps, index_name=\"TimeStamps\"):\n",
    "    \"\"\"\n",
    "    Function used to upsample the data corresponding to VideoID=1 \n",
    "    from 25Hz to 30Hz, so that it matches the sample frequency of the\n",
    "    data in the rest of the videos.\n",
    "\n",
    "    Given a dataframe `df`. It upsamples the numeric columns\n",
    "    doing linear interpolation based on a function trained on each column.\n",
    "    The non-numeric features are replaced by the same value of the \n",
    "    first row in the original df.ParticipantID\n",
    "    \n",
    "    Returns another pandas DataFrame, with the same columns than the input\n",
    "    `df` but the index corresponds to the `new_timestamps`.\n",
    "    \"\"\"\n",
    "    import scipy.interpolate\n",
    "\n",
    "    # Create new dataframe with the resampled version\n",
    "    df_resampled = pd.DataFrame(index=pd.Index(new_timestamps, name=index_name), columns=df.columns)\n",
    "\n",
    "    # Find the numeric columns that can be interpolated\n",
    "    cols_numeric = list(df.select_dtypes([np.number]).columns)\n",
    "    cols_non_numeric = list(set(df.columns.values).difference(set(cols_numeric)))\n",
    "\n",
    "    # Non-numeric columns are replaced with the first value in the original dataframe\n",
    "    df_resampled[cols_non_numeric] = df[cols_non_numeric].iloc[0,:]\n",
    "\n",
    "    # Apply interpolation\n",
    "    x = df.index.values#.total_seconds().values\n",
    "    y = df[cols_numeric].values\n",
    "    f_interpolation = scipy.interpolate.interp1d(x,y,axis=0)\n",
    "    df_resampled[cols_numeric] = f_interpolation(new_timestamps)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a reference TimeStamp array to be used in the resampled version of VideoID=1\n",
    "timestamps_reference = df_filtered_single_ts.set_index(ts_colname).dropna(axis=1, how=\"all\")\n",
    "timestamps_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dataframe from another video as reference for the timestamps required after resampling\n",
    "timestamps_reference = timestamps_reference.index.values\n",
    "timestamps_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size original dataset:{data_postprocessed.shape}\")\n",
    "\n",
    "# Subset of data with values 1 and remove it from original dataset\n",
    "data_video_1 = data_postprocessed[ data_postprocessed.VideoID ==1 ].copy(deep=True)\n",
    "print(f\"Size data video 1:{data_video_1.shape}\")\n",
    "\n",
    "# Delete from main dataset, we will input new values with proper resampling.\n",
    "data_postprocessed = data_postprocessed[ data_postprocessed.VideoID != 1 ]\n",
    "print(f\"Size after removing video 1:{data_postprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe for video 1\n",
    "data_video_1_resampled = None\n",
    "\n",
    "# Extract each time series (per participant, and per data group)\n",
    "for pid in data_video_1.ParticipantID.unique():\n",
    "    for dgroup in data_video_1.data_type.unique():\n",
    "        Q = ( (data_video_1.ParticipantID == pid) \n",
    "                & (data_video_1.data_type == dgroup) )\n",
    "        df_filter = data_video_1[ Q ]\n",
    "\n",
    "        # Define timestamps as the index, and delete \n",
    "        # columns that are not relevant to the datagroup\n",
    "        df_filter.set_index(ts_colname, inplace=True)\n",
    "        df_filter.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "        # Resample the df_filter with the same timestamps than the reference.\n",
    "        df_resampled = upsample_df_with_interpolation(df_filter, timestamps_reference, index_name=df_filter.index.name)\n",
    "        df_resampled.reset_index(inplace=True)\n",
    "\n",
    "        data_video_1_resampled = df_resampled if (data_video_1_resampled is None) else pd.concat([data_video_1_resampled, df_resampled], axis=0, ignore_index=True)\n",
    "        # print(f\"Original = {df_filter.shape} - Resampled = {df_resampled.shape}\")\n",
    "        # break\n",
    "    # break\n",
    "print(f\"Size data video 1 resampled: {data_video_1_resampled.shape}\\n\\tEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the resampled data from video 1 to the original dataset\n",
    "data_postprocessed = pd.concat([data_postprocessed, data_video_1_resampled], axis=0, ignore_index=True)\n",
    "print(f\"Size after inserting video 1 resampled:{data_postprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter.reset_index().plot(subplots=True, figsize=(9,5))\n",
    "df_resampled.reset_index().plot(subplots=True, figsize=(9,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Raw IBI to R-peaks array and attach to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_RAW_IBI_FILENAME = gen_path_temp(\"Dataset_CEAP_Physio_Raw\", extension=\".csv\")\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "data_IBI_raw = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [DATASET_RAW_IBI_FILENAME]\n",
    "\n",
    "# Try to load or create files\n",
    "for tries in range(3): # One for loading, and max 2 for creating+loading\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Try # {tries+1} to load files: {input_files}\")\n",
    "        \n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to READ from files\n",
    "        data_IBI_raw = pd.read_csv(input_files[0])\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "        #############################################################\n",
    "        break\n",
    "    except Exception as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to WRITE to disk\n",
    "\n",
    "        # Load all raw IBI to generate HRV data\n",
    "        for pid in PARTICIPANTS_IDS:\n",
    "            df_single_file = data_loader_ceap.load_data_from_participant(pid,\"Physio\",\"Raw\")\n",
    "            data_IBI_raw = df_single_file if (data_IBI_raw is None) else pd.concat([data_IBI_raw, df_single_file], axis=0)\n",
    "            \n",
    "        # Saving .csv\n",
    "        data_IBI_raw.to_csv( input_files[0], index=False)\n",
    "\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        print(f\"\\n\\tFinished creating files {input_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_peaks_from_IBI(df, FS = 30, ibi_colname=\"IBI_IBI\", output_colname=\"IBI_R_Peaks\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe `df` containing irregular physiological \n",
    "    features from interbeat intervals with column name:`IBI_IBI`.\n",
    "    This function returns another dataframe containing 60 seconds of\n",
    "    data at the same sampling rate than the rest of the dataset\n",
    "    preprocessed as `Frame`.\n",
    "\n",
    "    This series contains the position of the `peaks` as `1`, and\n",
    "    the rest of the array contains zeros. The returned dataframe can be\n",
    "    directly used directly in neurokit2 package to extract HRV features:\n",
    "     - `neurokit2.hrv(peaks, sampling_rate=FS)`\n",
    "    \"\"\"\n",
    "    # The first IBI allows to regenerate a new peak right after the first IBI\n",
    "    first_beat_time = df.index[0]\n",
    "    first_beat_time = first_beat_time - df.iloc[0][0]\n",
    "    if(first_beat_time>=0):\n",
    "        df.loc[first_beat_time] = 0\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Generate an zero-array that will contain the R-peaks as 1's at a specific sampling frequency `FPS`\n",
    "    MAXIMUM_TIME_SECS = 60\n",
    "    ts_index_resampled = np.linspace(0, MAXIMUM_TIME_SECS, 60 * FS) # The way used by the authors of the dataset. I would use `np.arange(0,60,1/FS)`\n",
    "    df_peaks = pd.DataFrame(data=np.zeros(ts_index_resampled.size, dtype=int), index=ts_index_resampled, columns=[\"IBI_IBI\"])\n",
    "\n",
    "    # Match the IBI times to the closest timestamp in the array containing the peaks\n",
    "    closest_times_to_peaks = df_peaks.index.get_indexer(df.index.values, method=\"nearest\")\n",
    "    closest_index_values = df_peaks.index[closest_times_to_peaks] # Get index values from the positions\n",
    "    df_peaks.loc[closest_index_values] = 1\n",
    "\n",
    "    # The dataframe needs to be called `R_Peaks` to extract HRV with neurokit\n",
    "    df_peaks = df_peaks.rename({ibi_colname:output_colname}, axis=1)\n",
    "    \n",
    "    return df_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing extraction of R_peaks from individual IBI\n",
    "\n",
    "# Find the participants with short amount of IBI data \n",
    "for pid in data_IBI_raw.ParticipantID.unique():\n",
    "    for vid in data_IBI_raw.VideoID.unique():\n",
    "        Q = ( (data_IBI_raw.ParticipantID == pid) \n",
    "            & (data_IBI_raw.VideoID == vid)\n",
    "        )\n",
    "        data_single_instance = data_IBI_raw[ Q ][ [ts_colname, ibi_colname] ].set_index(ts_colname).dropna(axis=0, how=\"all\")\n",
    "        if( ibi_colname in data_single_instance.columns and data_single_instance[ibi_colname].size > 0): # If contains the column, and the column has data\n",
    "            data_peaks_resampled = extract_peaks_from_IBI(data_single_instance, \n",
    "                                                        FS=RESAMPLING_FREQUENCY, \n",
    "                                                        ibi_colname=ibi_colname,\n",
    "                                                        output_colname=r_peaks_colname)\n",
    "        else:\n",
    "            data_peaks_resampled = pd.DataFrame({\n",
    "                ts_colname: timestamps_reference,\n",
    "                ibi_colname: np.zeros(timestamps_reference.size),\n",
    "            }).set_index(ts_colname)\n",
    "        \n",
    "        print(f\"P{pid}, V{vid}: SIZE:{data_single_instance.size} \\t\\tVALID: {data_single_instance.size >= THRESHOLD_SAMPLES_IBI} \\t R_peaks: {data_peaks_resampled.values.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example on how the peaks can be used in the feature extraction stage\n",
    "import neurokit2 as nk\n",
    "hrv_indices = nk.hrv(data_peaks_resampled, sampling_rate=RESAMPLING_FREQUENCY)#, show=True)\n",
    "hrv_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_WITH_RPEAKS_FILENAME = gen_path_temp(\"Dataset_CEAP_replacing_IBI_with_RPeaks\", extension=\".csv\")\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "data_postprocessed_with_Rpeaks = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [DATASET_POSTPROCESSED_WITH_RPEAKS_FILENAME]\n",
    "\n",
    "# Try to load or create files\n",
    "for tries in range(3): # One for loading, and max 2 for creating+loading\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Try # {tries+1} to load files: {input_files}\")\n",
    "        \n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to READ from files\n",
    "        data_postprocessed_with_Rpeaks = pd.read_csv(input_files[0])\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "        #############################################################\n",
    "        break\n",
    "    except Exception as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to WRITE to disk\n",
    "\n",
    "        data_postprocessed_with_Rpeaks = data_postprocessed\n",
    "\n",
    "        # Add empty R_Peaks to the whole dataset\n",
    "        data_postprocessed_with_Rpeaks[r_peaks_colname] = np.nan\n",
    "\n",
    "        # Iterate over participants and videos to add the respective R_peaks\n",
    "        for pid in np.sort(data_postprocessed_with_Rpeaks.ParticipantID.unique()):\n",
    "            for vid in np.sort(data_postprocessed_with_Rpeaks.VideoID.unique()):\n",
    "                #######\n",
    "                # Query to filter subset of IBI data\n",
    "                Q = ( (data_IBI_raw.ParticipantID == pid) \n",
    "                    & (data_IBI_raw.VideoID == vid)\n",
    "                    & (data_IBI_raw.data_type == \"Physio\"))\n",
    "                # Extract the R_peaks from the corresponding Raw IBI\n",
    "                data_single_instance = data_IBI_raw[Q][[ts_colname, ibi_colname]].set_index(ts_colname).dropna(axis=0, how=\"all\")\n",
    "                # If contains the column, and the column has data\n",
    "                if(ibi_colname in data_single_instance.columns and data_single_instance[ibi_colname].size > 0):\n",
    "                    data_peaks_resampled = extract_peaks_from_IBI(data_single_instance,\n",
    "                                                            FS=RESAMPLING_FREQUENCY,\n",
    "                                                            ibi_colname=ibi_colname,\n",
    "                                                            output_colname=r_peaks_colname)\n",
    "                else:\n",
    "                    # Dataframe full of zeros but without peaks, to compensate for those samples without IBI data.\n",
    "                    data_peaks_resampled = pd.DataFrame({\n",
    "                        ts_colname: timestamps_reference,\n",
    "                        r_peaks_colname: np.zeros(timestamps_reference.size, dtype=int),\n",
    "                    }).set_index(ts_colname)\n",
    "\n",
    "                #######\n",
    "                # Replace the relevant subsection of the postprocessed data\n",
    "                # Query to filter subset of big dataframe\n",
    "                Q = ((data_postprocessed_with_Rpeaks.ParticipantID == pid)\n",
    "                    & (data_postprocessed_with_Rpeaks.VideoID == vid)\n",
    "                    & (data_postprocessed_with_Rpeaks.data_type == \"Physio\"))\n",
    "                idx_to_replace = data_postprocessed_with_Rpeaks[Q].index\n",
    "                data_postprocessed_with_Rpeaks.loc[idx_to_replace, r_peaks_colname ] = data_peaks_resampled[r_peaks_colname].values\n",
    "                print(f\"P{pid} - V{vid} - #R-Peaks:{data_peaks_resampled[r_peaks_colname].values.sum()}\")\n",
    "\n",
    "                #######\n",
    "                # # Testing size of the dataset\n",
    "                # df_instance = data_postprocessed_with_Rpeaks[Q]\n",
    "                # df_instance[r_peaks_colname] = data_peaks_resampled.values\n",
    "\n",
    "                # # Get numeric and non-numeric colnames\n",
    "                # cols_numeric = list(df_instance.select_dtypes([np.number]).columns)\n",
    "                # cols_non_numeric = list( set(df_instance.columns.values).difference(set(cols_numeric)) )\n",
    "\n",
    "                # # print(f\"P:{pid} \\tV:{vid} \\tShape:{df_instance.shape} \\tCols:{cols_numeric}\")\n",
    "\n",
    "        # All the individual instances should be the same shape (1800) even after adding IBI_R_peaks\n",
    "        print(data_postprocessed_with_Rpeaks.shape)\n",
    "        \n",
    "        ## Saving .csv\n",
    "        data_postprocessed_with_Rpeaks.to_csv( input_files[0], index=False)\n",
    "\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        print(f\"\\n\\tFinished creating files {input_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Final dataset ready for feature extraction\n",
    "---\n",
    "\n",
    "The dataset ready to be used executes the following steps:\n",
    "\n",
    "1. Remove NaN by merging the time-series per their respective `data_group`. They already have the same `ParticipantID`, `VideoID` and `TimeStamp`, thus it's easy to remove the column that indicates the type of data (*Annotations, Behavior, Physio*) so that the whole dataframe does not contain missing values.\n",
    "2. Include the class labels in high/low arousal/valence according to the paper: `[HAHL, HALV, LAHV, LA,LV]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgroup_colname = \"data_type\"        # Existing column to be removed\n",
    "class_label_colname = \"class_VA\"    # Class column name to be created\n",
    "\n",
    "# These columns are used as index to join df\n",
    "basic_colnames = [participant_colname, video_colname, ts_colname]\n",
    "basic_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping used according to the paper's information in Table 1\n",
    "# doi: 10.1109/TMM.2021.3124080\n",
    "MAPPING_VIDEO_TO_CLASS = {\n",
    "    1: \"HVHA\",\n",
    "    2: \"HVLA\",\n",
    "    3: \"LVHA\",\n",
    "    4: \"LVLA\",\n",
    "    5: \"HVHA\",\n",
    "    6: \"HVLA\",\n",
    "    7: \"LVHA\",\n",
    "    8: \"LVLA\",\n",
    "}\n",
    "# Create a function from the dictionary to apply on the final array\n",
    "mapper_videoid_to_classes = np.vectorize(MAPPING_VIDEO_TO_CLASS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_WITHOUT_NAN = gen_path_temp(\"Dataset_CEAP_postprocessed\", extension=\".csv\")\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "dataset_postprocessed_no_nan = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [DATASET_POSTPROCESSED_WITHOUT_NAN]\n",
    "\n",
    "# Try to load or create files\n",
    "for tries in range(3): # One for loading, and max 2 for creating+loading\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Try # {tries+1} to load files: {input_files}\")\n",
    "        \n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to READ from files\n",
    "        data_postprocessed_with_Rpeaks = pd.read_csv(input_files[0])\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "        #############################################################\n",
    "        break\n",
    "    except Exception as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        #vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "        ##          Custom functions to WRITE to disk\n",
    "\n",
    "        # Delete preprocessing level info (Full of labels saying `Frame`)\n",
    "        data_postprocessed_with_Rpeaks.drop([\"processing_level\"], axis=1, inplace=True)\n",
    "\n",
    "        # Merge data from different groups to remove Nan values\n",
    "        for pid in np.sort(data_postprocessed_with_Rpeaks[participant_colname].unique()):\n",
    "            for vid in np.sort(data_postprocessed_with_Rpeaks[video_colname].unique()):\n",
    "                # Stores the different data groups per time-series instance.\n",
    "                df_instance = None\n",
    "                for dg in DATA_GROUPS:\n",
    "                    print(f\"P{pid} V{vid} G:{dg}\")\n",
    "                    Q = ( (data_postprocessed_with_Rpeaks[participant_colname] == pid) \n",
    "                        & (data_postprocessed_with_Rpeaks[video_colname] == vid)\n",
    "                        & (data_postprocessed_with_Rpeaks[dgroup_colname] == dg))\n",
    "                    selection_idx = data_postprocessed_with_Rpeaks[Q].index\n",
    "                    data_per_group = data_postprocessed_with_Rpeaks.loc[selection_idx].copy()\n",
    "                    # Load the data get the relevant columns that do not contain missing values\n",
    "                    data_per_group.drop(dgroup_colname, axis=1, inplace=True)\n",
    "                    data_per_group.set_index(basic_colnames, inplace=True)\n",
    "                    data_per_group.dropna(axis=1, how=\"all\", inplace=True)\n",
    "                    # Add specific data group to time series\n",
    "                    df_instance = data_per_group if (df_instance is None) else df_instance.join(data_per_group)\n",
    "\n",
    "                # Add joined dataset to general one\n",
    "                df_instance.reset_index(inplace=True)\n",
    "                dataset_postprocessed_no_nan = df_instance if (dataset_postprocessed_no_nan is None) else pd.concat([dataset_postprocessed_no_nan, df_instance], axis=0, ignore_index=True)\n",
    "        print(\"\\tEnd\")\n",
    "\n",
    "        # Map each video to the corresponding Class label\n",
    "        video_id_array = dataset_postprocessed_no_nan[video_colname]\n",
    "        dataset_postprocessed_no_nan[class_label_colname] = mapper_videoid_to_classes(video_id_array)\n",
    "\n",
    "        print(dataset_postprocessed_no_nan.shape)\n",
    "\n",
    "        # Saving .csv\n",
    "        dataset_postprocessed_no_nan.to_csv( input_files[0], index=False)\n",
    "\n",
    "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        print(f\"\\n\\tFinished creating files {input_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TODO`\n",
    "\n",
    "- [x] Load the big dataset with preprocessing `Frame`\n",
    "- [X] Creating summary plots for the whole dataset\n",
    "- [X] Transform the data from videoId 1 from 25Hz to 30Hz to be consistent with the rest.\n",
    "- [X] Extract the irregular `IBI` and transform it into regular array @30Hz containing peaks.\n",
    "- [X] Delete NaN from the columns by joining them by `data_type`\n",
    "- [X] Add class labels for Valence-Arousal according to the paper.\n",
    "- [X] Save the dataset with time-series @30Hz with index: `[ParticipantID, VideoID, DataGroup]` and values corresponding to features in the groups: `[Annotations, Behavior, Physio]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary below can be used to recover the column names per data type\n",
    "{\n",
    "    'Annotations': ['Valence', 'Arousal'],\n",
    "    'Behavior': ['HM_Pitch', 'HM_Yaw', 'EM_Pitch', 'EM_Yaw', 'LEM_Pitch', 'LEM_Yaw', 'REM_Pitch', 'REM_Yaw', 'LPD_PD', 'RPD_PD'],\n",
    "    'Physio': ['ACC_ACC_X', 'ACC_ACC_Y', 'ACC_ACC_Z', 'SKT_SKT', 'EDA_EDA', 'BVP_BVP', 'HR_HR', 'IBI_R_Peaks']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
